{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets\n",
    "This loads the iris dataset from ```sklearn.datasets``` and uses ```np.genfromtxt``` to load the ionosphere data from a text file. We need to split the information loaded from the file into the samples (data) and their labels (target). Each row represents a labelled sample; the first 34 columns are features, and the last column is the label of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris \n",
    "\n",
    "iris = load_iris()\n",
    "ionosphere_data =  np.genfromtxt(\"ionosphere.txt\", delimiter=\",\", usecols=np.arange(34))\n",
    "ionosphere_target = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\", usecols=34, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting datasets into training and test sets\n",
    "\n",
    "This splits our loaded iris and ionosphere datasets into training and test sets using ```train_test_split``` (75% training, 25% test). (Birthday is 0308, so random_state becomes 308 by omitting leading zeros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(iris['data'],\n",
    "                                                                        iris['target'], \n",
    "                                                                        random_state=308)\n",
    "\n",
    "# print(iris['data'][:3])\n",
    "# print(iris['target'][:3])\n",
    "\n",
    "X_train_ionosphere, X_test_ionosphere, y_train_ionosphere, y_test_ionosphere = train_test_split(ionosphere_data,\n",
    "                                                                                                ionosphere_target, \n",
    "                                                                                                random_state=308)\n",
    "\n",
    "# print(ionosphere_data[:3])\n",
    "# print(ionosphere_target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance/metric\n",
    "\n",
    "Now, to implement the nearest neighbour algorithm, we need to be able to measure the distance between data points. \n",
    "\n",
    "(Using the Euclidean distance/metric ||x âˆ’ x*||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def getDistance(training_sample, test_sample):\n",
    "   \n",
    "    distance = 0\n",
    "    for i, j in zip(training_sample, test_sample):\n",
    "        distance += (i - j) ** 2\n",
    "       \n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbour algorithm\n",
    "\n",
    "One Nearest Neighbour:\n",
    "   >Given a **training set** and **test sample**, we can predict that the *label of a test sample* will be the *same as the label of the nearest training sample* in the set (in terms of Euclidean distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneNN(training_set, training_labels, test_sample):\n",
    "    \n",
    "    \"\"\" Finds the distances between each of the training_labels in the training_set \n",
    "        and the test_sample. It then finds the minimum distance (which belongs to the \n",
    "        nearest training_sample in the test set) and uses that label as the predicted \n",
    "        label of the test sample. Returns a tuple consisting of the nearest neigbour, \n",
    "        (the training_sample), its distance from the test_sample and the predicted_label\"\"\"\n",
    "    \n",
    "    training_distances = {}\n",
    "    \n",
    "    for training_sample, label in zip(training_set, training_labels):\n",
    "\n",
    "        distance = getDistance(training_sample, test_sample)\n",
    "        training_distances[distance] = (training_sample, label)\n",
    "        \n",
    "    least_distance = min(training_distances)\n",
    "    nearest_neighbour, predicted_label = training_distances[least_distance]\n",
    "    \n",
    "    return nearest_neighbour, least_distance, predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode, StatisticsError\n",
    "import random\n",
    "\n",
    "def kNN(training_set, training_labels, test_sample, k):\n",
    "    \n",
    "    training_distances = {}\n",
    "    \n",
    "    for training_sample, label in zip(training_set, training_labels):\n",
    "\n",
    "        distance = getDistance(training_sample, test_sample)\n",
    "        training_distances[distance] = (training_sample, label)\n",
    "        \n",
    "    k_nearest_neighbours_labels = []\n",
    "    \n",
    "    for i in range(k): \n",
    "        least_distance = min(training_distances)\n",
    "        k_nearest_neighbours_labels.append(training_distances.pop(least_distance)[1])\n",
    "    \n",
    "    try:\n",
    "        predicted_label = mode(k_nearest_neighbours_labels)\n",
    "    \n",
    "    except StatisticsError:\n",
    "        predicted_label = random.choice(k_nearest_neighbours_labels)\n",
    "        print('Choosing randomly out of nearest neighbours due to tie in majority vote.')\n",
    "        \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2, 2, 2], 1.7320508075688772, 'a')\n",
      "(array([6.8, 3. , 5.5, 2.1]), 0.09999999999999964, 2)\n",
      "['a']\n",
      "a\n",
      "[2, 2, 2]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Test values\n",
    "\n",
    "ts = [[2, 2, 2],\n",
    "      [4, 4, 4]]\n",
    "\n",
    "tl = ['a', 'b']\n",
    "s = [1, 1, 1]\n",
    "print(oneNN(ts, tl, s))\n",
    "\n",
    "test_iris = np.array([6.7, 3, 5.5, 2.1])\n",
    "print(oneNN(iris['data'], iris['target'], test_iris))\n",
    "\n",
    "print(kNN(ts, tl, s, 1))\n",
    "print(kNN(iris['data'], iris['target'], test_iris, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions and finding the accuracy\n",
    "\n",
    "To make predictions, we need to find the nearest neighbour (and therefore the predicted label) for every test sample in the test set. Once we have a list of all the predicted labels, we can compare it to the actual labels for the test set; to get the accuracy, we find the ratio of the number of correct predictions out of the total predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(X_train, y_train, X_test):\n",
    "    \n",
    "    \"\"\" Makes and returns a list containing the predicted labels for each of the test_samples \n",
    "        in the X_test set. It predicts the labels by using the one Nearest Neighbour algorithm.\"\"\"\n",
    "    \n",
    "    predicted_labels = [\n",
    "        oneNN(X_train, y_train, test_sample)[2]\n",
    "        for test_sample in X_test\n",
    "    ]\n",
    "        \n",
    "    return predicted_labels\n",
    "\n",
    "def getAccuracy(predicted_labels, y_test):\n",
    "    \n",
    "    return np.mean(predicted_labels == y_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ionosphere_predictions = getPredictions(X_train_ionosphere, y_train_ionosphere, X_test_ionosphere)\n",
    "ionosphere_accuracy = getAccuracy(ionosphere_predictions, y_test_ionosphere)\n",
    "ionosphere_test_error = 1 - ionosphere_accuracy\n",
    "\n",
    "\n",
    "iris_predictions = getPredictions(X_train_iris, y_train_iris, X_test_iris)\n",
    "iris_accuracy = getAccuracy(iris_predictions, y_test_iris)\n",
    "iris_test_error = 1 - iris_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ionosphere test_error_rate: 14.772727%\n",
      "Iris test_error_rate: 5.263158%\n"
     ]
    }
   ],
   "source": [
    "print(\"Ionosphere test_error_rate: %f%%\" %(ionosphere_test_error * 100))\n",
    "print(\"Iris test_error_rate: %f%%\" %(iris_test_error * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ionosphere test_error_rate\n",
    "14.77272727272727%\n",
    "= 14.77%\n",
    "\n",
    "# Iris test_error_rate\n",
    "5.2631578947368474%\n",
    "= 5.26%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the conformity score\n",
    "\n",
    "Split the labelled samples by their class/label. Then find the distance of the nearest sample of a different class to the test sample, and the distance of the nearest sample of the same class and divide the two. This will be the conformity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2]\n",
      "[-1, 1]\n"
     ]
    }
   ],
   "source": [
    "def split_by_label(X_train, y_train):\n",
    "    \n",
    "    \"\"\" Divide the training set by its label.\n",
    "        Returns a dictionary of labels mapped to\n",
    "        a list of corresponding training_samples. \"\"\"\n",
    "    \n",
    "    labels = {}\n",
    "\n",
    "    for training_sample, label in zip(X_train, y_train):\n",
    "    \n",
    "        if label in labels:\n",
    "            labels[label].append(training_sample)\n",
    "\n",
    "        else:\n",
    "            labels[label] = [training_sample]\n",
    "            \n",
    "    return labels\n",
    "            \n",
    "ionosphere_labels = split_by_label(X_train_ionosphere, y_train_ionosphere)\n",
    "iris_labels = split_by_label(X_train_iris, y_train_iris)\n",
    "        \n",
    "print(list(iris_labels))\n",
    "print(list(ionosphere_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conformity_score(sample, label, labelled_samples):\n",
    "    \n",
    "    \"\"\" Finds and returns the nearest sample of a different class, \n",
    "        nearest sample of the same class and the conformity score, \n",
    "        found by nearest_different / nearest_same. \"\"\"\n",
    "    \n",
    "    # Get nearest sample of same class/label\n",
    "    nearest_same = min(\n",
    "        getDistance(training_sample, sample) \n",
    "        for training_sample in labelled_samples[label])\n",
    "    \n",
    "    # Get all different classes/labels\n",
    "    different_labels = list(labelled_samples)\n",
    "    different_labels.remove(label)\n",
    "    different_samples = []\n",
    "    \n",
    "    # Get all samples of a different class/label\n",
    "    for diff in different_labels:\n",
    "        different_samples += labelled_samples[diff]\n",
    "    \n",
    "    # Get nearest sample of different class/label\n",
    "    nearest_different = min(\n",
    "        getDistance(training_sample, sample) \n",
    "        for training_sample in different_samples)\n",
    "    \n",
    "    conformity_score = (nearest_different / nearest_same) if (nearest_same != 0) else(\n",
    "        0 if (nearest_different == 0) else math.inf )\n",
    "    \n",
    "    return nearest_different, nearest_same, conformity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConformityScores(X_train, y_train, test_sample, test_label):\n",
    "    \n",
    "    \"\"\" Loops through a training set and returns a list containing \n",
    "        the conformity scores for each of the samples in the set, \n",
    "        provided the training set and its training labels. \"\"\"\n",
    "    \n",
    "    conformity_scores = []\n",
    "    \n",
    "    # Add test sample and label to array for which conformity scores will be calculated\n",
    "    training_samples = np.append(X_train, [test_sample], axis=0)\n",
    "    training_labels = np.append(y_train, [test_label], axis=0)\n",
    "\n",
    "    for i in range(len(training_samples)):\n",
    "        \n",
    "        X_copy = list(training_samples.copy())\n",
    "        y_copy = list(training_labels.copy())\n",
    "        \n",
    "        sample = X_copy.pop(i)\n",
    "        label = y_copy.pop(i)\n",
    "\n",
    "        labelled_samples = split_by_label(X_copy, y_copy)\n",
    "\n",
    "        score = get_conformity_score(sample, label, labelled_samples)\n",
    "        conformity_scores.append(score[2])\n",
    "        \n",
    "    return conformity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8944271909999159, 1.5811388300841895, 2.5495097567963922, 1.4142135623730951, 0.7071067811865476, 1.0, 0.35355339059327373]\n"
     ]
    }
   ],
   "source": [
    "# Test values\n",
    "\n",
    "training_set = np.array([[0, 3],\n",
    "                    [2, 2],\n",
    "                    [3, 3],\n",
    "                    [-1, 1],\n",
    "                    [-1, -1],\n",
    "                    [0, 1]])\n",
    "\n",
    "training_labels = [1, 1, 1, -1, -1, -1]\n",
    "\n",
    "test_sample = [0, 0]\n",
    "pred_label = 1\n",
    "\n",
    "conformities = getConformityScores(training_set, training_labels, test_sample, pred_label)\n",
    "print(conformities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPValue(training_set, training_labels, test_sample, pred_label):\n",
    "    \n",
    "    \"\"\" Returns the p_value for the test sample by finding \n",
    "        its rank (using its conformity score). The rank is its \n",
    "        positionin the list of sorted conformity scores.\"\"\"\n",
    "    \n",
    "    conformities = getConformityScores(training_set, training_labels, test_sample, pred_label)\n",
    "    \n",
    "    if conformities[-1] != math.inf:\n",
    "        temp = conformities.copy()\n",
    "        temp.sort()\n",
    "        rank = temp.index(conformities[-1]) + 1\n",
    "        return rank / len(conformities)\n",
    "    \n",
    "    # If the conformity score is infinity, it will have the highest rank.\n",
    "    else:\n",
    "        return len(conformities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPValue(training_set, training_labels, test_sample, pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_false_p_value(X_train, X_test, y_train, y_test, label_space):\n",
    "    \n",
    "    \"\"\" Calculates the average false p_value given a \n",
    "        test and training set, and all possible labels. \"\"\"\n",
    "    \n",
    "    false_p_values = []\n",
    "    \n",
    "    for test_sample, test_label in zip(X_test, y_test):\n",
    "        \n",
    "        # Calculate all p_values for each sample\n",
    "        for label in label_space:\n",
    "            \n",
    "            # Only considering false p_values\n",
    "            if label != test_label:\n",
    "                p_value = getPValue(X_train, y_train, test_sample, label)\n",
    "                false_p_values.append(p_value)\n",
    "        \n",
    "    average_false_p_value = sum(false_p_values) / len(false_p_values)\n",
    "    return(average_false_p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average False P value of ionosphere\n",
    "\n",
    "0.058109504132231454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058109504132231454\n"
     ]
    }
   ],
   "source": [
    "ionosphere_labels = [1, -1]\n",
    "average_false_p_value_ionosphere = average_false_p_value(X_train_ionosphere, \n",
    "                                                         X_test_ionosphere, \n",
    "                                                         y_train_ionosphere, \n",
    "                                                         y_test_ionosphere, \n",
    "                                                         ionosphere_labels)\n",
    "print(average_false_p_value_ionosphere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average False P value of iris\n",
    "\n",
    "0.011760596180717292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011760596180717292\n"
     ]
    }
   ],
   "source": [
    "iris_labels = [0, 1, 2]\n",
    "average_false_p_value_iris = average_false_p_value(X_train_iris, \n",
    "                                                  X_test_iris, \n",
    "                                                  y_train_iris, \n",
    "                                                  y_test_iris, \n",
    "                                                  iris_labels)\n",
    "print(average_false_p_value_iris)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
